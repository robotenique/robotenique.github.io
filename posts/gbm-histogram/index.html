<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.68.3" />

    
    
    

<title>Histograms for efficient gradient boosting ‚Ä¢ Juliano Garcia</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Histograms for efficient gradient boosting"/>
<meta name="twitter:description" content="Gradient Boosting GBM or Gradient Boosting Machines are a form of machine learning algorithms based on additive models. The currently most widely known implementations of it are the XGBoost and LightGBM libraries, and they are common choices for modeling supervised learning problems based on structured data. Generally, the output model in boosting is created by iteratively building weak learners, where each new weak learner weights more the mispredictions of the last learner, and so on."/>

<meta property="og:title" content="Histograms for efficient gradient boosting" />
<meta property="og:description" content="Gradient Boosting GBM or Gradient Boosting Machines are a form of machine learning algorithms based on additive models. The currently most widely known implementations of it are the XGBoost and LightGBM libraries, and they are common choices for modeling supervised learning problems based on structured data. Generally, the output model in boosting is created by iteratively building weak learners, where each new weak learner weights more the mispredictions of the last learner, and so on." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://robotenique.github.io/posts/gbm-histogram/" />
<meta property="article:published_time" content="2020-08-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-08-26T00:00:00+00:00" /><meta property="og:site_name" content="Robotenique&#39;s Home" />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.692b3e0d6f373fc39176eb7072aa6d3b8ab41661dd3acb4de670c2ea4c8a7d45.css" integrity="sha256-aSs&#43;DW83P8ORdutwcqptO4q0FmHdOstN5nDC6kyKfUU=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
      CommonHTML: {
        linebreaks: {automatic: true}
      },
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
      extensions: ["AMSmath.js", "AMSsymbols.js"] }  
});

</script>


</head>


    <body class="theme-base-08 layout-reverse">
    
<div class="sidebar">
  <div class="container sidebar-sticky">
  
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://robotenique.github.io/">Juliano Garcia</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="https://robotenique.github.io/img/me.jpg" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
        
      
      <p class="site__description">
         Data &amp; Computer Scientist. Digressions about statistics, technology or anything else that comes in my mind. Opinions are my own. 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Juliano Garcia</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/portfolio/">
						<span>Projects</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/about/">
						<span>About Me</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	
	
	<a href="https://github.com/robotenique" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/juliano-garcia-oliveira" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	
	<a href="mailto:julianogarcia_1997@hotmail.com" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    

<div class="builtwith">
Built with <a href="https://gohugo.io">Hugo</a> ‚ù§Ô∏è <a href="https://github.com/htr3n/hyde-hyde">hyde-hyde</a>.
</div>


  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Histograms for efficient gradient boosting</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Aug 26, 2020
    
    
    
      
      
          in
          
          
              <a class="badge badge-category" href="/categories/computer-science">COMPUTER SCIENCE</a>
              
          
      
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/data-science">data science</a>
           
      
          <a class="badge badge-tag" href="/tags/programming">programming</a>
          
      
    
    
    <br/>
    <i class="fas fa-clock"></i> 6 min read
</div>


  </header>
  
  
  <div class="post">
    


<p align="center">
  <img
    src="/img/gbm.gif"
    alt="gbm"
    decoding="async"
  />
</p>

<h3 id="gradient-boosting">Gradient Boosting</h3>
<p>GBM or Gradient Boosting Machines are a form of machine learning algorithms based on additive models. The currently most widely known implementations of it are the XGBoost and LightGBM libraries, and they are common choices for modeling supervised learning problems based on structured data. Generally, the output model in boosting is created by iteratively building weak learners, where each new weak learner weights more the mispredictions of the last learner, and so on. Almost all of the current gradient boosting libraries implement these base weak learners as <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision trees</a>, and this is one of the reasons for the efficiency of these algorithms.</p>
<p>It can be thought of as a sum of multiple weak learners $F_m$ built on a stagewise fashion, where each $F$ is (in XGBoost and LightGBM) a decision tree. So if the model is trained with $M$ trees, the final model can be interpreted as:</p>
<p>$$ F_M(x) = F_0(x) + \sum_{m=1}^{M}F_m(x)$$</p>
<p>If you&rsquo;re interested in understanding more about the peculiarities of GBMs or why it usually gives good results, I highly recommend checking the first three chapters of my <a href="https://www.linux.ime.usp.br/~robotenique/mac0499/">undergraduate thesis</a> and the paper <a href="https://ntnuopen.ntnu.no/ntnu-xmlui/handle/11250/2433761">Why Does XGBoost Win &ldquo;Every&rdquo; Machine Learning Competition?</a>. XGBoost and LightGBM both introduced new and smart ways to make the algorithm computationally efficient while keeping approximately the accuracy. This post aims to showcase and give more intuition into one of these improvements, which is using histograms instead of feature values when finding the optimal splits for a given tree node. There are other improvements made as well, like GOSS and EFB (described more in-depth in the <a href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf">LightGBM paper</a>), but I won&rsquo;t tackle these two for now.</p>
<h3 id="the-bottleneck-of-a-decision-tree">The bottleneck of a decision tree</h3>
<p>Let&rsquo;s focus on the building process of a single tree $F_m$. The <strong>induction</strong> of decision trees can be seen as a <a href="https://en.wikipedia.org/wiki/Greedy_algorithm">Greedy Algorithm</a>, usually in a top-down approach where at each node of the tree a pair $(f, f_t)$ is chosen. The notation here is that $f$ is a feature and $f_t$ is a threshold value of the said feature. What the node represents can be interpreted as a question:</p>
<blockquote>
<p>&ldquo;is $f &lt; f_t$? if yes continue to the left child, else go to the right child&rdquo;</p>
</blockquote>



<p align="center">
  <img
    src="/img/gbm2.png"
    alt="gbm2"
    decoding="async"
  />
</p>

<p>In a decision tree, the objective is to find the best pair $(f, f_t)$ according to a metric of entropy, usually Information Gain or Gini Impurity. So, to put this split finding algorithm in simple terms:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">for</span> each feature f:
    <span style="color:#66d9ef">for</span> each feature threshold value f_t:
        compute gain <span style="color:#66d9ef">if</span> splitting this node using (f, f_t)

choose (f, f_t) <span style="color:#66d9ef">with</span> the best gain
</code></pre></div><p>In each node of the GBM algorithm, the <strong>gain</strong> only depends on the <strong>sum of gradients</strong> of the left child, the right child, and the current node. To calculate these sum of gradients fast, a typical approach is to <em>sort</em> the values of the feature being tested. For example, suppose all unique values of the feature <strong>&ldquo;age&rdquo;</strong> are $[30, 44, 10, 55, 8, 19, 22]$. These values are sorted into $[8, 10, 19, 22, 30, 44, 55]$ and sequentially test increasingly higher values of threshold $f_t$.</p>
<p>One might recall that for sorting algorithms that make comparisons, it isn&rsquo;t possible to achieve have better complexity than $\mathcal{O}(n\ log\ n)$. So the split finding process complexity can be approximated to $\mathcal{O}(|F|\cdot n\ log\ n)$, where |F| is the number of features that are being evaluated and $n$ the number of values to evaluate for each one of them (they could be different, but for simplicity purposes, I&rsquo;m assuming they&rsquo;re the same).</p>
<p>So, could this split finding algorithm be more efficient? ü§îü§î</p>
<h3 id="histogram-to-the-rescue">Histogram to the rescue</h3>
<p>This approach can be better understood thinking in terms of splitting continuous features, and the basic idea is to approximate the threshold values using some form of quantization. In LightGBM the algorithm creates a <em>histogram</em> of feature values, with equal bin density (i.e. transforms the feature distribution into a uniform distribution for split finding), built using a subset of the dataset (controlled by the <code>bin_construct_sample_cnt</code> parameter).</p>
<h4 id="gradient-binning">Gradient binning</h4>
<p>So, here&rsquo;s an example of how this works and why it improves performance. Consider this training set with just two features $f_1$ and $f_2$, represented in each column:</p>
<p>$$ \left[ \begin{matrix}
1.5 &amp; 0.0\\\<br>
0.0 &amp; 5.5\\\<br>
0.3 &amp; 7.0\\\<br>
5.5 &amp; 8.5
\end{matrix}  \right]$$</p>
<p>And then the equal density histogram for each feature is created, replacing each feature value with the <strong>index</strong> of the <em>bin</em> that value is mapped to! So if the histogram has 3 buckets/bins, this could be the output after the binning procedure:</p>
<p>$$ \left[ \begin{matrix}
1 &amp; 0\\\<br>
0 &amp; 1\\\<br>
0 &amp; 2\\\<br>
2 &amp; 2
\end{matrix}  \right]$$</p>
<p>But what information does the histogram have exactly? Recall that to compute the <strong>gain</strong> it&rsquo;s only necessary to have the sum of gradients for the left child, the right child, and the current node. So, this sum of gradients is stored in each bin of the histogram, which allows for faster computation of the gain. At first, the algorithm complexity was $\mathcal{O}(n\ log\ n)$, but by using histograms it&rsquo;s now reduced to $\mathcal{O}(n\_bins)$! In LightGBM $n\_bins=255$ (can be changed with the <code>max_bin</code> parameter), and this is one of the reasons for the huge performance increase of modern gradient boosting frameworks.</p>
<p>Each value maps to the index in the histogram ($g_i$ represent the sum of gradients from data inside bin $i$):</p>



<p align="center">
  <img
    src="/img/gbm3.png"
    alt="gbm3"
    decoding="async"
  />
</p>

<h4 id="subtraction-trick">Subtraction trick</h4>
<p>This is also a simple yet cool trick used when calculating the gain of a node in this splitting process. The gain function for a node is more complicated as it involves the hessian and regularization parameters, but let&rsquo;s simplify things a little bit. As assumed earlier, the left and right child gradients are enough to calculate the gain of a node:</p>
<p>$$\sum{g_i} = \sum_{k \in \text{ left}}{g_k} + \sum_{k \in \text{ right}}{g_j}$$</p>
<p>When considering a split value for a feature, the values are either mapped to the left or right child. So, the gradients follow this simple rule that $Histogram(parent) = Histogram(left) + Histogram(right)$, which means it&rsquo;s possible to calculate the gain/histogram of one of the child nodes and subtract it from the current node (which is already calculated), and the histogram of gradients for the other child is obtained &ldquo;automatically&rdquo;.</p>
<p>In practice, this reduces the histogram building process from $\mathcal{O}(n)$ (looking up all the data points in the current node to build the histogram ) to $\mathcal{O}(n\_bins)$ for <strong>half</strong> the cases due to the subtraction trick, as it isn&rsquo;t necessary to manually build the histogram for one of the children. Usually the algorithm calculates the histogram for the child with the smallest $n$ and get the other child histogram via the subtraction trick.</p>
<p>The following figure illustrates this: starting with the <em>Parent</em> histogram and calculating the <em>Left</em> histogram (blue), their subtraction results in the red histogram, which corresponds to the <em>Right</em> child histogram!</p>



<p align="center">
  <img
    src="/img/gbm4.png"
    alt="gbm4"
    decoding="async"
  />
</p>

<h3 id="references">References</h3>
<p>And that&rsquo;s it! By using an <em>approximate</em> solution the whole training process gradient boosting gets a huge <em>boost</em>üëÄ  in computational performance. This approach is being adopted and added to a lot of different frameworks too, e.g. you can check the code for the subtraction trick <a href="https://github.com/scikit-learn/scikit-learn/blob/ac8760d089b279abc70f12db247c59fdc4b0fcc0/sklearn/ensemble/_hist_gradient_boosting/histogram.pyx#L200">in this Cython code for Scikit-Learn</a>, and they even have a similar approach in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html"><code>HistGradientBoostingClassifier</code> and <code>HistGradientBoostingRegressor</code></a>, inspired by LightGBM!</p>
<p>Finally, the following parameters can be used to alter the default behavior of the histograms in LightGBM (use it carefully):</p>
<ul>
<li><code>bin_construct_sample_cnt</code></li>
<li><code>min_data_in_bin</code></li>
<li><code>data_random_seed</code></li>
<li><code>max_bin</code></li>
</ul>
<p>More details about each of them can be found in the <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html">LightGBM parameters documentation</a>.</p>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/posts/bezier-curve-gen/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Generative art using B√©zier Curves</span>
    </a>
    
    
    <a href="/posts/2017-03-10-python-map/" class="navigation-next">
      <span class="navigation-tittle">Python map</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.11.2/js/all.js" integrity="sha384-b3ua1l97aVGAPEIe48b4TC60WUQbQaGi2jqAWM90y0OZXZeyaTCWtBTKtjW2GXG1" crossorigin="anonymous"></script>


    
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
